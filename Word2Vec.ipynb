{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siting1206/NLP_HW1/blob/main/Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "nYewMEKGaB4E"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import namedtuple\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import regex as re\n",
        "import os, string, sys\n",
        "\n",
        "from gensim.models.word2vec import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "n6m5nZarbp_x",
        "outputId": "0293b595-7b0f-41f4-b89b-880aa6f70950",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "OUyDzynaaB4I"
      },
      "outputs": [],
      "source": [
        "class RegexFeatures(object):\n",
        "    PATTERNS = {\n",
        "        \"isInitCapitalWord\": re.compile(r'^[A-Z][a-z]+'),\n",
        "        \"isAllCapitalWord\": re.compile(r'^[A-Z][A-Z]+$'),\n",
        "        \"isAllSmallCase\": re.compile(r'^[a-z]+$'),\n",
        "        \"isWord\": re.compile(r'^[a-zA-Z][a-zA-Z]+$'),\n",
        "        \"isAlphaNumeric\": re.compile(r'^\\p{Alnum}+$'),\n",
        "        \"isSingleCapLetter\": re.compile(r'^[A-Z]$'),\n",
        "        \"containsDashes\": re.compile(r'.*--.*'),\n",
        "        \"containsDash\": re.compile(r'.*\\-.*'),\n",
        "        \"singlePunctuation\": re.compile(r'^\\p{Punct}$'),\n",
        "        \"repeatedPunctuation\": re.compile(r'^[\\.\\,!\\?\"\\':;_\\-]{2,}$'),\n",
        "        \"singleDot\": re.compile(r'[.]'),\n",
        "        \"singleComma\": re.compile(r'[,]'),\n",
        "        \"singleQuote\": re.compile(r'[\\']'),\n",
        "        \"isSpecialCharacter\": re.compile(r'^[#;:\\-/<>\\'\\\"()&]$'),\n",
        "        \"fourDigits\": re.compile(r'^\\d\\d\\d\\d$'),\n",
        "        \"isDigits\": re.compile(r'^\\d+$'),\n",
        "        \"isNumber\": re.compile(r'^((\\p{N}{,2}([,]?\\p{N}{3})+)(\\.\\p{N}+)?)$'),\n",
        "        \"containsDigit\": re.compile(r'.*\\d+.*'),\n",
        "        \"endsWithDot\": re.compile(r'\\p{Alnum}+\\.$'),\n",
        "        \"isURL\": re.compile(r'^http[s]?://'),\n",
        "        \"isMention\": re.compile(r'^(RT)?@[\\p{Alnum}_]+$'),\n",
        "        \"isHashtag\": re.compile(r'^#\\p{Alnum}+$'),\n",
        "        \"isMoney\": re.compile(r'^\\$((\\p{N}{,2}([,]?\\p{N}{3})+)(\\.\\p{N}+)?)$'),\n",
        "    }\n",
        "    def __init__(self):\n",
        "        print(\"Initialized RegexFeature\")\n",
        "    def process(word):\n",
        "        features = dict()\n",
        "        for k, p in RegexFeatures.PATTERNS.iteritems():\n",
        "            if p.match(word):\n",
        "                features[k] = True\n",
        "        return features\n",
        "\n",
        "\n",
        "Tag = namedtuple(\"Tag\", [\"token\", \"tag\"])\n",
        "\n",
        "def load_sequences(filename, sep=\"\\t\", notypes=False, test_data=False):\n",
        "    sequences = []\n",
        "    with open(filename) as fp:\n",
        "        seq = []\n",
        "        for line in fp:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                line = line.split(sep)\n",
        "                seq.append(Tag(*line))\n",
        "            else:\n",
        "                sequences.append(seq)\n",
        "                seq = []\n",
        "        if seq:\n",
        "            sequences.append(seq)\n",
        "    return sequences\n",
        "\n",
        "def load_test_sequences(filename, sep=\"\\t\"):\n",
        "    sequences = []\n",
        "    with open(filename) as fp:\n",
        "        seq = []\n",
        "        for line in fp:\n",
        "          line = line.strip()\n",
        "          if line != \".\":\n",
        "            seq.append(line)\n",
        "          else:\n",
        "            sequences.append(seq)\n",
        "            seq = []\n",
        "        if seq:\n",
        "          sequences.append(seq)\n",
        "    return sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rgQ-Ku-aaB4L"
      },
      "outputs": [],
      "source": [
        "train_sequences = load_sequences(\"drive/MyDrive/NLP_assignment1/data/train.txt\", sep=\"\\t\", notypes=True)\n",
        "dev_sequences = load_sequences(\"drive/MyDrive/NLP_assignment1/data/dev.txt\", sep=\"\\t\", notypes=False)\n",
        "\n",
        "test_sequences = load_test_sequences(\"drive/MyDrive/NLP_assignment1/data/test-submit.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "i6Nv9TccaB4M"
      },
      "outputs": [],
      "source": [
        "train_sentences = [[t[0] for t in seq] for seq in (train_sequences)]\n",
        "train_tags = [[t[1] for t in seq] for seq in (train_sequences)]\n",
        "\n",
        "valid_sentences = [[t[0] for t in seq] for seq in (dev_sequences)]\n",
        "valid_tags = [[t[1] for t in seq] for seq in (dev_sequences)]\n",
        "\n",
        "# print(train_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "other_entities = {\n",
        "    \"isHashtag\": [],\n",
        "    \"isMention\": [],\n",
        "    \"isURL\": [],\n",
        "    \"isMoney\": [],\n",
        "    \"isNumber\": [],\n",
        "    \"repeatedPunctuation\": []\n",
        "}\n",
        "for seq in train_sentences:\n",
        "    for t in seq:\n",
        "        for k in other_entities.keys():\n",
        "            if RegexFeatures.PATTERNS[k].match(t):\n",
        "                other_entities[k].append(t)\n",
        "for k, v in other_entities.items():\n",
        "    print(k, len(v))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eo2dvfaVj9-",
        "outputId": "500ead56-87ff-43fe-9d1c-9e92d056af08"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "isHashtag 440\n",
            "isMention 1292\n",
            "isURL 448\n",
            "isMoney 5\n",
            "isNumber 120\n",
            "repeatedPunctuation 1059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ENTITY_MAPPINGS={k: \"__%s__\" % k for k in other_entities.keys()}\n",
        "ENTITY_MAPPINGS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf-tWWMmVGVv",
        "outputId": "34f45f79-ee16-4c9f-cd87-f909c4603390"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'isHashtag': '__isHashtag__',\n",
              " 'isMention': '__isMention__',\n",
              " 'isURL': '__isURL__',\n",
              " 'isMoney': '__isMoney__',\n",
              " 'isNumber': '__isNumber__',\n",
              " 'repeatedPunctuation': '__repeatedPunctuation__'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "Zm-Sln_vaB4O"
      },
      "outputs": [],
      "source": [
        "def preprocess_token(x, to_lower=False):\n",
        "    for k in ENTITY_MAPPINGS.keys():\n",
        "        if RegexFeatures.PATTERNS[k].match(x):\n",
        "            return ENTITY_MAPPINGS[k]\n",
        "    if to_lower:\n",
        "        x = x.lower()\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### For the input of LSTM model all the sentences must be padded to same length,for that we must know the maximum length of the sequence in the list of sentences."
      ],
      "metadata": {
        "id": "MSYkXqJPyUbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pre_seq = [[preprocess_token(t[0], to_lower=False) for t in seq] for seq in train_sequences]\n",
        "test_pre_seq = [[preprocess_token(t, to_lower=False) for t in seq] for seq in test_sequences]\n",
        "# print(train_pre_seq)"
      ],
      "metadata": {
        "id": "9e82vQ0lz7QI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_sentences = [preprocess_token(t[0], to_lower=False) for seq in train_sequences for t in seq]\n",
        "tag2vec_sentences = [t[1] for seq in train_sequences for t in seq]\n",
        "words=list(set(word2vec_sentences))\n",
        "# print(word2vec_sentences)\n",
        "tags=list(set(tag2vec_sentences))\n",
        "# print(tags)\n",
        "w_index={t:j for j,t in enumerate(words)}\n",
        "t_index={t:j for j,t in enumerate(tags)}\n",
        "n_words = len(w_index)\n",
        "n_tags = len(t_index)\n",
        "y_train = [[t_index[w[1]] for w in s] for s in train_sequences]"
      ],
      "metadata": {
        "id": "RbSFcBOeHwYk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "id": "T4QtqogMaB4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d40b017-39e4-42fe-869b-2eeeef1c4389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequence length in the list of sentences: 36\n"
          ]
        }
      ],
      "source": [
        "maxl = max([len(s) for s in word2vec_sentences])\n",
        "\n",
        "print ('Maximum sequence length in the list of sentences:', maxl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "etsMMY-9aB4P",
        "outputId": "cc5f9019-61a3-483e-fd18-1968bb0aa11b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'__isMention__'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "word2vec_sentences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XusiZhG6aB4Q",
        "outputId": "70a8bcfb-0b33-49b6-b045-ebfcee444505",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'__isMention__'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "preprocess_token(\"@guild_gamer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QeeN9Tj9aB4Q",
        "outputId": "0f3775f3-2b18-4af8-d19a-7301e32acee1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec(vocab=8534, size=50, alpha=0.025)\n"
          ]
        }
      ],
      "source": [
        "word2vec = Word2Vec(train_pre_seq, size=50, window=10, sg=1, hs=0, min_count=1, negative=5, workers=1, iter=5)\n",
        "print(word2vec)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def word_to_vec(word):\n",
        "  try:\n",
        "    wordvec = word2vec[word]\n",
        "  except KeyError as e:\n",
        "    print(word, \"不存在\")\n",
        "    wordvec = np.array([0], * 100)\n",
        "  return wordvec\n",
        "\n",
        "X_train = [word_to_vec(s[0]) for s in train_pre_seq]\n",
        "# X_test = [word_to_vec(s[0]) for s in test_pre_seq]\n",
        "# print(X_train)\n"
      ],
      "metadata": {
        "id": "P-C9WmxRWWW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab5a9147-f99a-4dcf-830c-b7af2366ad1f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"總共收錄了 {len(word2vec.wv.vocab)} 個詞彙\")\n",
        "print(\"印出 20 個收錄詞彙:\")\n",
        "print(list(word2vec.wv.vocab.keys())[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk9hRjzsiwsR",
        "outputId": "61bec1fa-9a33-4d76-949c-0a37bdf516e7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "總共收錄了 8534 個詞彙\n",
            "印出 20 個收錄詞彙:\n",
            "['__isMention__', 'they', 'will', 'be', 'all', 'done', 'by', 'Sunday', 'trust', 'me', '*wink*', 'Made', 'it', 'back', 'home', 'to', 'GA', '.', 'It', 'sucks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec.wv.most_similar(\"good\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HSMsAMWjlqD",
        "outputId": "da22024f-b303-45ec-a59d-d103bff71c45"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('now', 0.9969407320022583),\n",
              " ('too', 0.996832013130188),\n",
              " ('need', 0.9968186616897583),\n",
              " ('all', 0.9965500235557556),\n",
              " ('gonna', 0.9963576197624207),\n",
              " ('have', 0.9960732460021973),\n",
              " ('one', 0.995962917804718),\n",
              " ('had', 0.9959287643432617),\n",
              " ('much', 0.9957548975944519),\n",
              " ('work', 0.9954919815063477)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "jqx_c2RV-8cS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pad_sequences(maxlen=maxl,padding='post',sequences=X_train)\n",
        "y_train = pad_sequences(maxlen=maxl,padding='post',sequences=y_train)\n",
        "print(X_train.shape, y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGByFbLt-82c",
        "outputId": "acc122c2-ebec-4d12-8f18-46cf35cefec8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2394, 36) (2394, 36)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}