{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siting1206/NLP_HW1/blob/main/Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "collapsed": true,
        "id": "nYewMEKGaB4E"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import namedtuple\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import regex as re\n",
        "import os, string, sys\n",
        "\n",
        "from gensim.models.word2vec import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "n6m5nZarbp_x",
        "outputId": "820b9ecc-779e-45b5-93f9-c4318273bfab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "drWvT6iy_Zxu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "collapsed": true,
        "id": "OUyDzynaaB4I"
      },
      "outputs": [],
      "source": [
        "class RegexFeatures(object):\n",
        "    PATTERNS = {\n",
        "        \"repeatedPunctuation\": re.compile(r'^[\\.\\,!\\?\"\\':;_\\-]{2,}$'),\n",
        "        \"isNumber\": re.compile(r'^((\\p{N}{,2}([,]?\\p{N}{3})+)(\\.\\p{N}+)?)$'),\n",
        "        \"isURL\": re.compile(r'^http[s]?://'),\n",
        "        \"isMention\": re.compile(r'^(RT)?@[\\p{Alnum}_]+$'),\n",
        "        \"isHashtag\": re.compile(r'^#\\p{Alnum}+$'),\n",
        "        \"isMoney\": re.compile(r'^\\$((\\p{N}{,2}([,]?\\p{N}{3})+)(\\.\\p{N}+)?)$'),\n",
        "    }\n",
        "    def __init__(self):\n",
        "        print(\"Initialized RegexFeature\")\n",
        "    def process(word):\n",
        "        features = dict()\n",
        "        for k, p in RegexFeatures.PATTERNS.iteritems():\n",
        "            if p.match(word):\n",
        "                features[k] = True\n",
        "        return features\n",
        "\n",
        "\n",
        "Tag = namedtuple(\"Tag\", [\"token\", \"tag\"])\n",
        "\n",
        "def load_sequences(filename, sep=\"\\t\", notypes=False, test_data=False):\n",
        "    sequences = []\n",
        "    with open(filename) as fp:\n",
        "        seq = []\n",
        "        for line in fp:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                line = line.split(sep)\n",
        "                seq.append(Tag(*line))\n",
        "            else:\n",
        "                sequences.append(seq)\n",
        "                seq = []\n",
        "        if seq:\n",
        "            sequences.append(seq)\n",
        "    return sequences\n",
        "\n",
        "def load_test_sequences(filename, sep=\"\\t\"):\n",
        "    sequences = []\n",
        "    with open(filename) as fp:\n",
        "        seq = []\n",
        "        for line in fp:\n",
        "          line = line.strip()\n",
        "          if line != \".\":\n",
        "            seq.append(line)\n",
        "          else:\n",
        "            sequences.append(seq)\n",
        "            seq = []\n",
        "        if seq:\n",
        "          sequences.append(seq)\n",
        "    return sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Dataset"
      ],
      "metadata": {
        "id": "v0pZGXTG_iNm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "rgQ-Ku-aaB4L"
      },
      "outputs": [],
      "source": [
        "train_sequences = load_sequences(\"drive/MyDrive/NLP_assignment1/data/train.txt\", sep=\"\\t\", notypes=True)\n",
        "dev_sequences = load_sequences(\"drive/MyDrive/NLP_assignment1/data/dev.txt\", sep=\"\\t\", notypes=False)\n",
        "\n",
        "test_sequences = load_test_sequences(\"drive/MyDrive/NLP_assignment1/data/test-submit.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "collapsed": true,
        "id": "i6Nv9TccaB4M"
      },
      "outputs": [],
      "source": [
        "train_sentences = [[t[0] for t in seq] for seq in (train_sequences)]\n",
        "train_tags = [[t[1] for t in seq] for seq in (train_sequences)]\n",
        "\n",
        "valid_sentences = [[t[0] for t in seq] for seq in (dev_sequences)]\n",
        "valid_tags = [[t[1] for t in seq] for seq in (dev_sequences)]\n",
        "\n",
        "# print(train_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "other_entities = {\n",
        "    \"isHashtag\": [],\n",
        "    \"isMention\": [],\n",
        "    \"isURL\": [],\n",
        "    \"isMoney\": [],\n",
        "    \"isNumber\": [],\n",
        "    \"repeatedPunctuation\": []\n",
        "}\n",
        "for seq in train_sentences:\n",
        "    for t in seq:\n",
        "        for k in other_entities.keys():\n",
        "            if RegexFeatures.PATTERNS[k].match(t):\n",
        "                other_entities[k].append(t)\n",
        "for k, v in other_entities.items():\n",
        "    print(k, len(v))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eo2dvfaVj9-",
        "outputId": "e56d3158-c2b6-4c6c-83d0-beb3adbe3956"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "isHashtag 440\n",
            "isMention 1292\n",
            "isURL 448\n",
            "isMoney 5\n",
            "isNumber 120\n",
            "repeatedPunctuation 1059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ENTITY_MAPPINGS={k: \"__%s__\" % k for k in other_entities.keys()}\n",
        "ENTITY_MAPPINGS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf-tWWMmVGVv",
        "outputId": "201ba643-19d9-4f32-95b4-fa181d7a8b12"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'isHashtag': '__isHashtag__',\n",
              " 'isMention': '__isMention__',\n",
              " 'isURL': '__isURL__',\n",
              " 'isMoney': '__isMoney__',\n",
              " 'isNumber': '__isNumber__',\n",
              " 'repeatedPunctuation': '__repeatedPunctuation__'}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "collapsed": true,
        "id": "Zm-Sln_vaB4O"
      },
      "outputs": [],
      "source": [
        "def preprocess_token(x, to_lower=False):\n",
        "    for k in ENTITY_MAPPINGS.keys():\n",
        "        if RegexFeatures.PATTERNS[k].match(x):\n",
        "            return ENTITY_MAPPINGS[k]\n",
        "    if to_lower:\n",
        "        x = x.lower()\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### For the input of LSTM model all the sentences must be padded to same length,for that we must know the maximum length of the sequence in the list of sentences."
      ],
      "metadata": {
        "id": "MSYkXqJPyUbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pre_seq = [[preprocess_token(t[0], to_lower=False) for t in seq] for seq in train_sequences]\n",
        "test_pre_seq = [[preprocess_token(t, to_lower=False) for t in seq] for seq in test_sequences]\n",
        "# print(train_pre_seq)"
      ],
      "metadata": {
        "id": "9e82vQ0lz7QI"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_sentences = [preprocess_token(t[0], to_lower=False) for seq in train_sequences for t in seq]\n",
        "tag2vec_sentences = [t[1] for seq in train_sequences for t in seq]\n",
        "words=list(set(word2vec_sentences))\n",
        "# print(word2vec_sentences)\n",
        "tags=list(set(tag2vec_sentences))\n",
        "# print(tags)\n",
        "w_index={t:j for j,t in enumerate(words)}\n",
        "t_index={t:j for j,t in enumerate(tags)}\n",
        "n_words = len(w_index)\n",
        "n_tags = len(t_index)\n",
        "y_train = [[t_index[w[1]] for w in s] for s in train_sequences]"
      ],
      "metadata": {
        "id": "RbSFcBOeHwYk"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "collapsed": true,
        "id": "T4QtqogMaB4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "403774b5-aebb-42c6-ea3b-5066919913a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequence length in the list of sentences: 36\n"
          ]
        }
      ],
      "source": [
        "maxl = max([len(s) for s in word2vec_sentences])\n",
        "\n",
        "print ('Maximum sequence length in the list of sentences:', maxl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "etsMMY-9aB4P",
        "outputId": "4d5187d9-ca26-47a4-9588-d04a0488463f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'__isMention__'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "word2vec_sentences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "XusiZhG6aB4Q",
        "outputId": "7cbc9e27-66c4-4f5d-da02-ae914c9ac29f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'__isMention__'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "preprocess_token(\"@guild_gamer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec Model"
      ],
      "metadata": {
        "id": "XwgGF1Fh_qni"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "QeeN9Tj9aB4Q",
        "outputId": "ea09f049-fc75-4357-df9e-b29d0999cca6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec(vocab=8534, size=50, alpha=0.025)\n"
          ]
        }
      ],
      "source": [
        "word2vec = Word2Vec(train_pre_seq, size=50, window=10, sg=1, hs=0, min_count=1, negative=5, workers=1, iter=5)\n",
        "print(word2vec)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def word_to_vec(word):\n",
        "  try:\n",
        "    wordvec = word2vec[word]\n",
        "  except KeyError as e:\n",
        "    print(word, \"不存在\")\n",
        "    wordvec = np.array([0], * 100)\n",
        "  return wordvec\n",
        "\n",
        "X_train = [word_to_vec(s[0]) for s in train_pre_seq]\n",
        "# X_test = [word_to_vec(s[0]) for s in test_pre_seq]\n",
        "# print(X_train)\n"
      ],
      "metadata": {
        "id": "P-C9WmxRWWW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c20ee642-5c27-4399-9757-860c3b7c5e09"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"總共收錄了 {len(word2vec.wv.vocab)} 個詞彙\")\n",
        "print(\"印出 20 個收錄詞彙:\")\n",
        "print(list(word2vec.wv.vocab.keys())[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk9hRjzsiwsR",
        "outputId": "4986b06c-699d-4c6a-8ba5-b9346c28d2f9"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "總共收錄了 8534 個詞彙\n",
            "印出 20 個收錄詞彙:\n",
            "['__isMention__', 'they', 'will', 'be', 'all', 'done', 'by', 'Sunday', 'trust', 'me', '*wink*', 'Made', 'it', 'back', 'home', 'to', 'GA', '.', 'It', 'sucks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec.wv.most_similar(\"Sunday\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HSMsAMWjlqD",
        "outputId": "20c141fa-b47d-493f-9d2f-dde3eb0b7b2a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('please', 0.9987372756004333),\n",
              " ('more', 0.9983861446380615),\n",
              " ('new', 0.99811851978302),\n",
              " ('later', 0.998092532157898),\n",
              " ('weekend', 0.9980342984199524),\n",
              " ('done', 0.99791020154953),\n",
              " ('our', 0.9977797865867615),\n",
              " ('awesome', 0.9976701736450195),\n",
              " ('meet', 0.9975164532661438),\n",
              " ('start', 0.9973775148391724)]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "jqx_c2RV-8cS"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pad_sequences(maxlen=maxl,padding='post',sequences=X_train)\n",
        "y_train = pad_sequences(maxlen=maxl,padding='post',sequences=y_train)\n",
        "print(X_train.shape, y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGByFbLt-82c",
        "outputId": "b183a530-be62-4d62-9564-6e99f906e257"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2394, 36) (2394, 36)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}